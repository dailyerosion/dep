"""Place jobs into our DEP queue!"""

import datetime
import json
import os
import time

import click
import httpx
import pandas as pd
import pika
from pyiem.database import get_sqlalchemy_conn, sql_helper
from pyiem.util import logger

from pydep.workflows.wepprun import (
    WeppJobPayload,
    WeppRunConfig,
    build_runfile,
)

YEARS = datetime.date.today().year - 2006
WEPPEXE = "wepp20240930"

# These are HUC12s that we currently need graph output for SWEEP
GRAPH_HUC12 = (
    "090201081101 090201081102 090201060605 102702040203 101500041202 "
    "090203010403 070200070501 070102050503 090203030703 090203030702"
).split()


def get_rabbitmqconn() -> tuple[pika.BlockingConnection, dict[str, str]]:
    """Load the configuration."""
    # load rabbitmq.json in the directory local to this script
    with open("rabbitmq.json", "r", encoding="utf-8") as fh:
        config = json.load(fh)
    return (
        pika.BlockingConnection(
            pika.ConnectionParameters(
                host=config["host"],
                port=config["port"],
                virtual_host=config["vhost"],
                credentials=pika.credentials.PlainCredentials(
                    config["user"], config["password"]
                ),
            )
        ),
        config,
    )


@click.command()
@click.option("-s", "--scenario", type=int, help="Scenario ID", default=0)
@click.option(
    "--runerrors", is_flag=True, help="Run previous runs that errored."
)
@click.option("--myhucs", help="Specify file of HUC12s to filter job.")
@click.option("--queue", help="RabbitMQ destination", default="dep")
def main(scenario: int, runerrors: bool, myhucs: str | None, queue: str):
    """Go main Go."""
    log = logger()
    if myhucs:
        log.warning("Using %s to filter job submission", myhucs)
        with open(myhucs, encoding="ascii") as fh:
            myhucs = [s.strip() for s in fh]

    with get_sqlalchemy_conn("idep") as conn:
        # Figure out the source of flowpaths
        res = conn.execute(
            sql_helper(
                "SELECT flowpath_scenario from scenarios where id = :scenario"
            ),
            {"scenario": scenario},
        )
        flscenario = res.fetchone()[0]

        flowpathdf = pd.read_sql(
            sql_helper(
                """
        SELECT huc_12, fpath, filepath, ofe_count, irrigated
        from flowpaths f JOIN climate_files c on (f.climate_file_id = c.id)
        where f.scenario = :flscenario {huclimit}
        """,
                huclimit=" and huc_12 = ANY(:hucs)" if myhucs else "",
            ),
            conn,
            params={"flscenario": flscenario, "hucs": myhucs},
        )
    totaljobs = len(flowpathdf.index)
    connection, rabbit_config = get_rabbitmqconn()
    channel = connection.channel()
    # Declare queue as durable (survives broker restart)
    # This is idempotent - safe to declare multiple times
    channel.queue_declare(queue=queue, durable=True)
    sts = datetime.datetime.now()

    weppconfig = WeppRunConfig(years=YEARS)

    for row in flowpathdf.itertuples():
        errfn = (
            f"/i/0/error/{row.huc_12[:8]}/{row.huc_12[8:]}"
            f"/{row.huc_12}_{row.fpath}.error"
        )
        if runerrors:
            if not os.path.isfile(errfn):
                continue
            os.unlink(errfn)

        # One offs
        weppconfig.enable_graph_file = row.huc_12 in GRAPH_HUC12

        payload = WeppJobPayload(
            errorfn=errfn,
            wepprun=build_runfile(
                weppconfig,
                f"/i/{scenario}/{{prefix}}/"
                f"{row.huc_12[:8]}/{row.huc_12[8:]}/"
                f"{row.huc_12}_{row.fpath}.{{prefix}}",
                f"/i/{scenario}/{{prefix}}/"
                f"{row.huc_12[:8]}/{row.huc_12[8:]}/"
                f"{row.huc_12}_{row.fpath}.{{prefix}}",
                row.filepath,
                f"/i/{scenario}/irrigation/ofe{row.ofe_count}.txt",
            ),
            weppexe=WEPPEXE,
        )
        # Publish to default exchange ("") with routing_key=queue name
        # This directly routes the message to the named queue
        channel.basic_publish(
            exchange="",  # Default exchange (nameless exchange)
            routing_key=queue,  # Queue name to route to
            body=payload.model_dump_json(),
            properties=pika.BasicProperties(
                # Message survives broker restart
                delivery_mode=pika.DeliveryMode.Persistent,
            ),
        )
    # Wait a few seconds for the dust to settle
    time.sleep(10)
    connection.close()
    percentile = 1.0001
    while True:
        now = datetime.datetime.now()
        req = httpx.get(
            f"http://{rabbit_config['host']}:15672/api/queues/%2F/{queue}",
            auth=(rabbit_config["user"], rabbit_config["password"]),
            timeout=60,
        )
        queueinfo = req.json()
        # jobs either ready or unawked
        jobsleft = queueinfo["messages_persistent"]
        done = totaljobs - jobsleft
        if (jobsleft / float(totaljobs)) < percentile:
            log.warning(
                "%6i/%s [%.3f /s]",
                jobsleft,
                totaljobs,
                done / (now - sts).total_seconds(),
            )
            percentile -= 0.1
        if (now - sts).total_seconds() > 36000:
            log.error("ERROR, 10 Hour Job Limit Hit")
            break
        if jobsleft == 0:
            log.warning("Done!")
            break
        time.sleep(30)


if __name__ == "__main__":
    main()
